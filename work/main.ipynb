{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "382AgpJgbOBM"
      },
      "source": [
        "# Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fwv-zQ1eNLaE"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQXO8iRBLa0P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator, AutoMinorLocator\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from IPython import display\n",
        "import time\n",
        "import json\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8vakXvP-qsP"
      },
      "source": [
        "## Support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Kkd9NH6g9fgr",
        "outputId": "b9c0e242-bf78-4d6b-92d6-09ced82b1921"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.rc('font', size=30)  # controls default text sizes\n",
        "plt.rc('axes', titlesize=25)  # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=25)  # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=17)  # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=17)  # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=20)  # legend fontsize\n",
        "plt.rc('figure', titlesize=30)\n",
        "plt.tight_layout()\n",
        "\n",
        "def plot(V, pi):\n",
        "    # plot value\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12.5, 5))\n",
        "    ax1.axis('on')\n",
        "    ax1.cla()\n",
        "    states = np.arange(V.shape[0])\n",
        "    ax1.bar(states, V, edgecolor='none')\n",
        "    ax1.set_xlabel('State')\n",
        "    ax1.set_ylabel('Value', rotation='horizontal', ha='right')\n",
        "    ax1.set_title('Value Function')\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True, nbins=6))\n",
        "    ax1.yaxis.grid()\n",
        "    ax1.set_ylim(bottom=V.min())\n",
        "    # plot policy\n",
        "    ax2.axis('on')\n",
        "    ax2.cla()\n",
        "    im = ax2.imshow(pi.T, cmap='Greys', vmin=0, vmax=1, aspect='auto')\n",
        "    ax2.invert_yaxis()\n",
        "    ax2.set_xlabel('State')\n",
        "    ax2.set_ylabel('Action', rotation='horizontal', ha='right')\n",
        "    ax2.set_title('Policy')\n",
        "    start, end = ax2.get_xlim()\n",
        "    ax2.xaxis.set_ticks(np.arange(start, end), minor=True)\n",
        "    ax2.xaxis.set_major_locator(MaxNLocator(integer=True, nbins=6))\n",
        "    ax2.yaxis.set_major_locator(MaxNLocator(integer=True, nbins=6))\n",
        "    start, end = ax2.get_ylim()\n",
        "    ax2.yaxis.set_ticks(np.arange(start, end), minor=True)\n",
        "    ax2.grid(which='minor')\n",
        "    divider = make_axes_locatable(ax2)\n",
        "    cax = divider.append_axes('right', size='5%', pad=0.20)\n",
        "    cbar = fig.colorbar(im, cax=cax, orientation='vertical')\n",
        "    cbar.set_label('Probability', rotation=0, ha='left')\n",
        "    fig.subplots_adjust(wspace=0.5)\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(fig)\n",
        "    time.sleep(0.001)\n",
        "    plt.close()\n",
        "\n",
        "class Transitions(list):\n",
        "\n",
        "    def __init__(self, transitions):\n",
        "        self.__transitions = transitions\n",
        "        super().__init__(transitions)\n",
        "\n",
        "    def __repr__(self):\n",
        "        repr = '{:<14} {:<10} {:<10}'.format('Next State', 'Reward',\n",
        "                                             'Probability')\n",
        "        repr += '\\n'\n",
        "        for i, (s, r, p) in enumerate(self.__transitions):\n",
        "            repr += '{:<14} {:<10} {:<10}'.format(s, round(r, 2), round(p, 2))\n",
        "            if i != len(self.__transitions) - 1:\n",
        "                repr += '\\n'\n",
        "        return repr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "madjT_LqazTi"
      },
      "source": [
        "# Environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3Di_2CMLVje"
      },
      "source": [
        "## GridWorld"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwH0AJAINDnj"
      },
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "\n",
        "    def __init__(self, size, initial_value=None):\n",
        "        self.size = size\n",
        "        self.states = np.arange(size * size)\n",
        "        self.actions = np.arange(4)\n",
        "        self.grid = np.full((size, size), initial_value)\n",
        "\n",
        "    def transitions(self, s, a):\n",
        "        return np.array([[r, self.p(s_, r, s, a)]\n",
        "                         for s_, r in self.support(s, a)])\n",
        "\n",
        "    def support(self, s, a):\n",
        "        return [(s_, self.reward(s, s_)) for s_ in self.states]\n",
        "\n",
        "    def p(self, next_state, reward, state, action):\n",
        "        row = state // self.size\n",
        "        col = state % self.size\n",
        "\n",
        "        if (state == 0 and next_state == 0):\n",
        "            return 1.0\n",
        "\n",
        "        if (state == len(self.states) - 1 and next_state == len(self.states) - 1):\n",
        "            return 1.0\n",
        "\n",
        "        if (state == 0 or state == len(self.states) - 1):\n",
        "            return 0.0\n",
        "\n",
        "        #up = 0\n",
        "        if (action == 0):\n",
        "            new_col = col\n",
        "            new_row = max(row - 1, 0)\n",
        "\n",
        "        #down = 1\n",
        "        if (action == 1):\n",
        "            new_col = col\n",
        "            new_row = min(row + 1, self.size - 1)\n",
        "\n",
        "        #left = 2\n",
        "        if (action == 2):\n",
        "            new_col = max(col - 1, 0)\n",
        "            new_row = row\n",
        "\n",
        "        #right = 3\n",
        "        if (action == 3):\n",
        "            new_col = min(col + 1, self.size - 1)\n",
        "            new_row = row\n",
        "\n",
        "        new_state = new_row * self.size + new_col\n",
        "        if (new_state == next_state):\n",
        "            return 1.0\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "    def reward(self, state, next_state):\n",
        "        if (state == 0 or state == len(self.states) - 1):\n",
        "            return 0.0\n",
        "        else:\n",
        "            return -1.0\n",
        "\n",
        "    @property\n",
        "    def A(self):\n",
        "        return list(self.actions)\n",
        "\n",
        "    @property\n",
        "    def S(self):\n",
        "        return list(self.states)\n",
        "\n",
        "    def is_done(self, state):\n",
        "        return True if state == 0 or state == 15 else False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "\n",
        "class GridWorldEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom Grid World environment.\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, grid_size=(4, 4), start_pos=(0, 0), goal_pos=(3, 3), obstacles=[(1, 1), (2, 2), (3, 3)]):\n",
        "        super(GridWorldEnv, self).__init__()\n",
        "\n",
        "        # Define the action space (up, down, left, right)\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        # Define the observation space (grid size)\n",
        "        self.observation_space = spaces.Box(low=0, high=grid_size[0]-1, shape=(2,), dtype=np.int64)\n",
        "\n",
        "        # Set the grid size\n",
        "        self.grid_size = grid_size\n",
        "\n",
        "        # Set the start and goal positions\n",
        "        self.start_pos = start_pos\n",
        "        self.goal_pos = goal_pos\n",
        "\n",
        "        # Set the obstacles\n",
        "        self.obstacles = obstacles\n",
        "\n",
        "        # Initialize the agent's position\n",
        "        self.agent_pos = start_pos\n",
        "\n",
        "        # Initialize the reward\n",
        "        self.reward = 0\n",
        "\n",
        "        # Initialize the done flag\n",
        "        self.done = False\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Perform an action in the environment.\n",
        "\n",
        "        Args:\n",
        "            action (int): The action to perform (0: up, 1: down, 2: left, 3: right).\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The new state (agent's position).\n",
        "            float: The reward for the action.\n",
        "            bool: Whether the episode has ended.\n",
        "            dict: Additional information (empty).\n",
        "        \"\"\"\n",
        "        # Get the current position\n",
        "        x, y = self.agent_pos\n",
        "\n",
        "        # Update the position based on the action\n",
        "        if action == 0:  # up\n",
        "            new_x, new_y = x, y + 1\n",
        "        elif action == 1:  # down\n",
        "            new_x, new_y = x, y - 1\n",
        "        elif action == 2:  # left\n",
        "            new_x, new_y = x - 1, y\n",
        "        else:  # right\n",
        "            new_x, new_y = x + 1, y\n",
        "\n",
        "        # Check if the new position is within the grid and not an obstacle\n",
        "        if 0 <= new_x < self.grid_size[0] and 0 <= new_y < self.grid_size[1] and (new_x, new_y) not in self.obstacles:\n",
        "            self.agent_pos = (new_x, new_y)\n",
        "\n",
        "\n",
        "        next_state = self.agent_pos[0] * self.grid_size[0] + self.agent_pos[1]\n",
        "\n",
        "        # Check if the agent has reached the goal\n",
        "        if self.agent_pos in self.goal_pos:\n",
        "            self.reward = 0\n",
        "            self.done = True\n",
        "        else:\n",
        "            self.reward = -1\n",
        "            self.done = False\n",
        "\n",
        "        return next_state, self.reward, self.done, {}, {}\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Reset the environment to the initial state.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The initial state (agent's position).\n",
        "        \"\"\"\n",
        "        self.agent_pos = self.start_pos\n",
        "        self.reward = 0\n",
        "        self.done = False\n",
        "\n",
        "        state = self.agent_pos[0] * self.grid_size[0]  + self.agent_pos[1]\n",
        "\n",
        "        return state\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        \"\"\"\n",
        "        Render the environment.\n",
        "\n",
        "        Args:\n",
        "            mode (str): The rendering mode ('human' or 'rgb_array').\n",
        "        \"\"\"\n",
        "        # Create a grid to represent the environment\n",
        "        grid = np.zeros((self.grid_size[0], self.grid_size[1]), dtype=str)\n",
        "\n",
        "        # Set the goal position\n",
        "        for goal in self.goal_pos:\n",
        "            grid[goal] = 'G'\n",
        "\n",
        "        # Set the obstacles\n",
        "        for obstacle in self.obstacles:\n",
        "            grid[obstacle] = 'X'\n",
        "\n",
        "        # Set the agent's position\n",
        "        grid[self.agent_pos] = 'A'\n",
        "\n",
        "        # Print the grid\n",
        "        print(grid)\n",
        "\n",
        "    @property\n",
        "    def A(self):\n",
        "        return self.action_space\n",
        "\n",
        "    @property\n",
        "    def S(self):\n",
        "        return self.observation_space"
      ],
      "metadata": {
        "id": "YQcJ-RxYQMbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW7j-pySag5e"
      },
      "source": [
        "# Dynamic Progamming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_Gm5RvNLVjW"
      },
      "source": [
        "## Policy Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltsKm8qxLVjZ"
      },
      "outputs": [],
      "source": [
        "def bellman_update(env, V, pi, s, gamma):\n",
        "    sum_v = 0.0\n",
        "    for a in env.A:\n",
        "        transitions = env.transitions(s, a)\n",
        "        for next_state in env.S:\n",
        "            reward = transitions[next_state, 0]\n",
        "            transition_prob = transitions[next_state, 1]\n",
        "            sum_v += pi[s][a] * transition_prob * (reward + gamma * V[next_state])\n",
        "    V[s] = sum_v\n",
        "\n",
        "def evaluate_policy(env, V, pi, gamma, theta):\n",
        "    delta = float('inf')\n",
        "    while delta > theta:\n",
        "        delta = 0\n",
        "        for s in env.S:\n",
        "            v = V[s]\n",
        "            bellman_update(env, V, pi, s, gamma)\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "\n",
        "    return V\n",
        "\n",
        "def evaluate_policy2(env, V, pi, gamma, steps):\n",
        "    for k in range(steps):\n",
        "        for s in env.S:\n",
        "            v = V[s]\n",
        "            bellman_update(env, V, pi, s, gamma)\n",
        "    return V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPFikRglLVjc"
      },
      "source": [
        "## Policy Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPMUrjc0LVjc"
      },
      "outputs": [],
      "source": [
        "def q_greedify_policy(env, V, pi, s, gamma):\n",
        "    q_pi = np.zeros(len(env.A))\n",
        "    for a in env.A:\n",
        "        transitions = env.transitions(s, a)\n",
        "        for next_state in env.S:\n",
        "            reward = transitions[next_state, 0]\n",
        "            transition_prob = transitions[next_state, 1]\n",
        "            q_pi[a] += transition_prob * (reward + gamma * V[next_state])\n",
        "    best_action = np.argmax(q_pi)\n",
        "    pi[s] = np.eye(len(env.A))[best_action]\n",
        "\n",
        "\n",
        "def improve_policy(env, V, pi, gamma):\n",
        "    policy_stable = True\n",
        "    for s in env.S:\n",
        "        old = pi[s].copy()\n",
        "        q_greedify_policy(env, V, pi, s, gamma)\n",
        "\n",
        "        if not np.array_equal(pi[s], old):\n",
        "            policy_stable = False\n",
        "\n",
        "    return pi, policy_stable\n",
        "\n",
        "def policy_iteration(env, gamma, theta):\n",
        "    V = np.zeros(len(env.S))\n",
        "    pi = np.ones((len(env.S), len(env.A))) / len(env.A)\n",
        "    policy_stable = False\n",
        "\n",
        "    while not policy_stable:\n",
        "        V = evaluate_policy(env, V, pi, gamma, theta)\n",
        "        pi, policy_stable = improve_policy(env, V, pi, gamma)\n",
        "\n",
        "    return V, pi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH_mPIK5LVjd"
      },
      "source": [
        "## Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX1lTcEvLVjd"
      },
      "outputs": [],
      "source": [
        "def bellman_optimality_update(env, V, s, gamma):\n",
        "    v = np.zeros(len(env.A))\n",
        "\n",
        "    for a in env.A:\n",
        "        transitions = env.transitions(s, a)\n",
        "        for next_state in env.S:\n",
        "            reward = transitions[next_state, 0]\n",
        "            transition_prob = transitions[next_state, 1]\n",
        "            v[a] += transition_prob * (reward + gamma * V[next_state])\n",
        "    V[s] = np.max(v)\n",
        "\n",
        "def value_iteration(env, gamma, theta):\n",
        "    V = np.zeros(len(env.S))\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in env.S:\n",
        "            v = V[s]\n",
        "            bellman_optimality_update(env, V, s, gamma)\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "    pi = np.ones((len(env.S), len(env.A))) / len(env.A)\n",
        "    for s in env.S:\n",
        "        q_greedify_policy(env, V, pi, s, gamma)\n",
        "    return V, pi\n",
        "\n",
        "def value_iteration2(env, gamma, theta):\n",
        "    V = np.zeros(len(env.S))\n",
        "    pi = np.ones((len(env.S), len(env.A))) / len(env.A)\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in env.S:\n",
        "            v = V[s]\n",
        "            q_greedify_policy(env, V, pi, s, gamma)\n",
        "            bellman_update(env, V, pi, s, gamma)\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V, pi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqMcrILeWjIm"
      },
      "source": [
        "# Monte Carlo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rq8D-I9tXu_M"
      },
      "outputs": [],
      "source": [
        "def first_visit_mc_prediction(env, policy, num_episodes, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Performs first-visit Monte Carlo prediction to estimate the value function.\n",
        "\n",
        "    Args:\n",
        "        env (gym.Env): The environment representing the MDP.\n",
        "        policy (function): A function that maps states to actions.\n",
        "        num_episodes (int): The number of episodes to simulate.\n",
        "        gamma (float): The discount factor.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary mapping states to their estimated values.\n",
        "    \"\"\"\n",
        "    # Initialize the value function\n",
        "    V = {}\n",
        "\n",
        "    # Initialize the returns for each state\n",
        "    returns = {}\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        # Reset the environment\n",
        "        state = env.reset()\n",
        "\n",
        "        # Generate an episode using the given policy\n",
        "        episode = []\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy(state)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            episode.append((state, reward))\n",
        "            state = next_state\n",
        "\n",
        "        # Calculate the return for each state in the episode\n",
        "        G = 0.0\n",
        "        t = len(episode) - 1\n",
        "        for state, reward in reversed(episode):\n",
        "            G = gamma * G + reward\n",
        "            if state not in [s for s, _ in episode[:t - 1]]:\n",
        "                if state not in returns:\n",
        "                    returns[state] = []\n",
        "                returns[state].append(G)\n",
        "            t -= 1\n",
        "\n",
        "    # Calculate the average return for each state\n",
        "\n",
        "    for state, total_returns in returns.items():\n",
        "        V[state] = np.mean(total_returns)\n",
        "\n",
        "    return V"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Temporal Difference\n"
      ],
      "metadata": {
        "id": "9tFVKiHX8RW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TD(0)"
      ],
      "metadata": {
        "id": "KP_54C_F8paR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iM4o0RtY8URr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NePtuEuNUAU"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rn_lKDu1Pex"
      },
      "source": [
        "## ENV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCL1Y03LeYzr",
        "outputId": "f4cbc3ca-f952-4010-da6f-bf5fccbdc5b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qw-sSXUvhUZ2"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "\n",
        "class GridWorldEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom Grid World environment.\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, grid_size=(4, 4), start_pos=(0, 0), goal_pos=(3, 3), obstacles=[(1, 1), (2, 2), (3, 3)]):\n",
        "        super(GridWorldEnv, self).__init__()\n",
        "\n",
        "        # Define the action space (up, down, left, right)\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        # Define the observation space (grid size)\n",
        "        self.observation_space = spaces.Box(low=0, high=grid_size[0]-1, shape=(2,), dtype=np.int64)\n",
        "\n",
        "        # Set the grid size\n",
        "        self.grid_size = grid_size\n",
        "\n",
        "        # Set the start and goal positions\n",
        "        self.start_pos = start_pos\n",
        "        self.goal_pos = goal_pos\n",
        "\n",
        "        # Set the obstacles\n",
        "        self.obstacles = obstacles\n",
        "\n",
        "        # Initialize the agent's position\n",
        "        self.agent_pos = start_pos\n",
        "\n",
        "        # Initialize the reward\n",
        "        self.reward = 0\n",
        "\n",
        "        # Initialize the done flag\n",
        "        self.done = False\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Perform an action in the environment.\n",
        "\n",
        "        Args:\n",
        "            action (int): The action to perform (0: up, 1: down, 2: left, 3: right).\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The new state (agent's position).\n",
        "            float: The reward for the action.\n",
        "            bool: Whether the episode has ended.\n",
        "            dict: Additional information (empty).\n",
        "        \"\"\"\n",
        "        # Get the current position\n",
        "        x, y = self.agent_pos\n",
        "\n",
        "        # Update the position based on the action\n",
        "        if action == 0:  # up\n",
        "            new_x, new_y = x, y + 1\n",
        "        elif action == 1:  # down\n",
        "            new_x, new_y = x, y - 1\n",
        "        elif action == 2:  # left\n",
        "            new_x, new_y = x - 1, y\n",
        "        else:  # right\n",
        "            new_x, new_y = x + 1, y\n",
        "\n",
        "        # Check if the new position is within the grid and not an obstacle\n",
        "        if 0 <= new_x < self.grid_size[0] and 0 <= new_y < self.grid_size[1] and (new_x, new_y) not in self.obstacles:\n",
        "            self.agent_pos = (new_x, new_y)\n",
        "\n",
        "\n",
        "        next_state = self.agent_pos[0] * self.grid_size[0] + self.agent_pos[1]\n",
        "\n",
        "        # Check if the agent has reached the goal\n",
        "        if self.agent_pos in self.goal_pos:\n",
        "            self.reward = 0\n",
        "            self.done = True\n",
        "        else:\n",
        "            self.reward = -1\n",
        "            self.done = False\n",
        "\n",
        "        return next_state, self.reward, self.done, {}, {}\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Reset the environment to the initial state.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The initial state (agent's position).\n",
        "        \"\"\"\n",
        "        self.agent_pos = self.start_pos\n",
        "        self.reward = 0\n",
        "        self.done = False\n",
        "\n",
        "        state = self.agent_pos[0] * self.grid_size[0]  + self.agent_pos[1]\n",
        "\n",
        "        return state\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        \"\"\"\n",
        "        Render the environment.\n",
        "\n",
        "        Args:\n",
        "            mode (str): The rendering mode ('human' or 'rgb_array').\n",
        "        \"\"\"\n",
        "        # Create a grid to represent the environment\n",
        "        grid = np.zeros((self.grid_size[0], self.grid_size[1]), dtype=str)\n",
        "\n",
        "        # Set the goal position\n",
        "        for goal in self.goal_pos:\n",
        "            grid[goal] = 'G'\n",
        "\n",
        "        # Set the obstacles\n",
        "        for obstacle in self.obstacles:\n",
        "            grid[obstacle] = 'X'\n",
        "\n",
        "        # Set the agent's position\n",
        "        grid[self.agent_pos] = 'A'\n",
        "\n",
        "        # Print the grid\n",
        "        print(grid)\n",
        "\n",
        "    @property\n",
        "    def A(self):\n",
        "        return self.action_space\n",
        "\n",
        "    @property\n",
        "    def S(self):\n",
        "        return self.observation_space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPCiqPdn1U71"
      },
      "source": [
        "## MC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1QiK9bTLenZj"
      },
      "outputs": [],
      "source": [
        "def first_visit_mc_prediction(env, policy, num_episodes, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Performs first-visit Monte Carlo prediction to estimate the value function.\n",
        "\n",
        "    Args:\n",
        "        env (gym.Env): The environment representing the MDP.\n",
        "        policy (function): A function that maps states to actions.\n",
        "        num_episodes (int): The number of episodes to simulate.\n",
        "        gamma (float): The discount factor.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary mapping states to their estimated values.\n",
        "    \"\"\"\n",
        "    # Initialize the value function\n",
        "    V = {}\n",
        "\n",
        "    # Initialize the returns for each state\n",
        "    returns = {}\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        # Reset the environment\n",
        "        state = env.reset()\n",
        "\n",
        "        # Generate an episode using the given policy\n",
        "        episode = []\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy(state)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            episode.append((state, reward))\n",
        "            state = next_state\n",
        "\n",
        "        # Calculate the return for each state in the episode\n",
        "        G = 0.0\n",
        "        t = len(episode) - 1\n",
        "        for state, reward in reversed(episode):\n",
        "            G = gamma * G + reward\n",
        "            if state not in [s for s, _ in episode[:t - 1]]:\n",
        "                if state not in returns:\n",
        "                    returns[state] = []\n",
        "                returns[state].append(G)\n",
        "            t -= 1\n",
        "\n",
        "    # Calculate the average return for each state\n",
        "    for state, total_returns in returns.items():\n",
        "        V[state] = np.mean(total_returns)\n",
        "\n",
        "    return V"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MCExploringStarts:\n",
        "    def __init__(self, env, gamma=0.99, epsilon=0.1, alpha=0.1):\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "        self.returns = defaultdict(list)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return self.env.action_space.sample()\n",
        "        else:\n",
        "            return np.argmax(self.Q[state])\n",
        "\n",
        "    def update(self, episode):\n",
        "        G = 0\n",
        "        for state, action, reward in reversed(episode):\n",
        "            G = reward + self.gamma * G\n",
        "            if (state, action) not in [(s, a) for s, a, _ in episode[:-1]]:\n",
        "                self.returns[(state, action)].append(G)\n",
        "                self.Q[state][action] = np.mean(self.returns[(state, action)])\n",
        "\n",
        "    def train(self, num_episodes):\n",
        "        for _ in range(num_episodes):\n",
        "            episode = []\n",
        "            state = self.env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.select_action(state)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                episode.append((state, action, reward))\n",
        "                state = next_state\n",
        "            self.update(episode)\n",
        "\n",
        "# Example usage\n",
        "env = gym.make('CartPole-v1')\n",
        "agent = MCExploringStarts(env)\n",
        "agent.train(num_episodes=10000)"
      ],
      "metadata": {
        "id": "dhxna9vnFu_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Monte Carlo Exploring Starts algorithm\n",
        "class MonteCarloES:\n",
        "    def __init__(self, env, gamma=1.0, epsilon=0.1):\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.q_values = {}\n",
        "        self.returns = {}\n",
        "\n",
        "        for i in range(env.size):\n",
        "            for j in range(env.size):\n",
        "                for action in env.get_possible_actions():\n",
        "                    self.q_values[((i, j), action)] = 0.0\n",
        "                    self.returns[((i, j), action)] = []\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice(self.env.get_possible_actions())\n",
        "        else:\n",
        "            q_values = [self.q_values[(state, a)] for a in self.env.get_possible_actions()]\n",
        "            return np.argmax(q_values)\n",
        "\n",
        "    def generate_episode(self):\n",
        "        episode = []\n",
        "        state = (random.randint(0, self.env.size-1), random.randint(0, self.env.size-1))\n",
        "        self.env.state = state\n",
        "\n",
        "        while True:\n",
        "            action = self.choose_action(state)\n",
        "            next_state, reward, done = self.env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            if done:\n",
        "                break\n",
        "            state = next_state\n",
        "\n",
        "        return episode\n",
        "\n",
        "    def update_q_values(self, episode):\n",
        "        g = 0\n",
        "        for t in range(len(episode)-1, -1, -1):\n",
        "            state, action, reward = episode[t]\n",
        "            g = self.gamma * g + reward\n",
        "            if (state, action) not in [(x[0], x[1]) for x in episode[:t]]:\n",
        "                self.returns[(state, action)].append(g)\n",
        "                self.q_values[(state, action)] = np.mean(self.returns[(state, action)])\n",
        "\n",
        "    def train(self, episodes=1000):\n",
        "        for _ in range(episodes):\n",
        "            episode = self.generate_episode()\n",
        "            self.update_q_values(episode)\n",
        "\n",
        "# Initialize environment and agent\n",
        "gridworld = Gridworld(size=5, start=(0, 0), goal=(4, 4))\n",
        "mc_agent = MonteCarloES(gridworld)\n",
        "\n",
        "# Train the agent\n",
        "mc_agent.train(episodes=10000)\n",
        "\n",
        "# Print Q-values\n",
        "for state_action, value in mc_agent.q_values.items():\n",
        "    print(f\"State-Action: {state_action}, Value: {value:.2f}\")"
      ],
      "metadata": {
        "id": "HVCI-3L4Fvri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initialize Q-table\n",
        "Q = np.zeros((num_states, num_actions))\n",
        "\n",
        "# Define exploration policy\n",
        "epsilon = 0.1\n",
        "\n",
        "# Implement Monte Carlo Exploring Starts\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        # Choose action based on epsilon-greedy policy\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.choice(num_actions)\n",
        "        else:\n",
        "            action = np.argmax(Q[state])\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Update Q-value based on the reward received\n",
        "        Q[state][action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action])\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "def mc_exploring_starts(env, num_episodes, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Performs first-visit Monte Carlo prediction to estimate the value function.\n",
        "\n",
        "    Args:\n",
        "        env (gym.Env): The environment representing the MDP.\n",
        "        policy (function): A function that maps states to actions.\n",
        "        num_episodes (int): The number of episodes to simulate.\n",
        "        gamma (float): The discount factor.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary mapping states to their estimated values.\n",
        "    \"\"\"\n",
        "\n",
        "    policy = np.random.rand(3,2)\n",
        "    Q = np.zeros((num_states, num_actions))\n",
        "    returns = np.zeros((num_states, num_actions))\n",
        "\n",
        "    # Initialize the returns for each state\n",
        "    returns = {}\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        # Reset the environment\n",
        "        state = env.reset()\n",
        "\n",
        "        # Generate an episode using the given policy\n",
        "        episode = []\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy(state)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            episode.append((state, reward))\n",
        "            state = next_state\n",
        "\n",
        "        # Calculate the return for each state in the episode\n",
        "        G = 0.0\n",
        "        t = len(episode) - 1\n",
        "        for state, reward in reversed(episode):\n",
        "            G = gamma * G + reward\n",
        "            if state not in [s for s, _ in episode[:t - 1]]:\n",
        "                if state not in returns:\n",
        "                    returns[state] = []\n",
        "                returns[state].append(G)\n",
        "            t -= 1\n",
        "\n",
        "    # Calculate the average return for each state\n",
        "    for state, total_returns in returns.items():\n",
        "        V[state] = np.mean(total_returns)\n",
        "\n",
        "    return V"
      ],
      "metadata": {
        "id": "qsHkA_SnF01C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f-dVO0Z1YD-"
      },
      "source": [
        "## RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TBGm-YIXhz3H",
        "outputId": "47782d7e-5751-4260-b419-c6e08c1d0cf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "int64\n",
            "Box(0, 3, (2,), int64)\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nstate = env.reset()\\n\\n# Render the environment\\nenv.render()\\n\\n# Define a random policy\\ndef policy(state):\\n    return env.action_space.sample()\\n\\n# Run the first-visit Monte Carlo prediction\\nvalue_function = first_visit_mc_prediction(env, policy, num_episodes=50000)\\n\\nprint(value_function)\\n\\n\\n# Create a grid to represent the environment\\ngrid = np.zeros((4, 4))\\n\\nfor state, total_returns in value_function.items():\\n    row = state // 4\\n    col = state % 4\\n    grid[row, col] = total_returns\\n\\n# Print the grid\\nprint(grid)\\n\\n'"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create the grid world environment\n",
        "env = GridWorldEnv(start_pos=(3, 0), goal_pos=[(0,0), (3,3)], obstacles=[])\n",
        "\n",
        "print(env.A.dtype)\n",
        "print(env.S)\n",
        "\n",
        "'''\n",
        "state = env.reset()\n",
        "\n",
        "# Render the environment\n",
        "env.render()\n",
        "\n",
        "# Define a random policy\n",
        "def policy(state):\n",
        "    return env.action_space.sample()\n",
        "\n",
        "# Run the first-visit Monte Carlo prediction\n",
        "value_function = first_visit_mc_prediction(env, policy, num_episodes=50000)\n",
        "\n",
        "print(value_function)\n",
        "\n",
        "\n",
        "# Create a grid to represent the environment\n",
        "grid = np.zeros((4, 4))\n",
        "\n",
        "for state, total_returns in value_function.items():\n",
        "    row = state // 4\n",
        "    col = state % 4\n",
        "    grid[row, col] = total_returns\n",
        "\n",
        "# Print the grid\n",
        "print(grid)\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TwLiU_qeJjeD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lD-TXAYheVEH",
        "outputId": "3eff543e-e99d-4e3d-b03e-c5efffa7fdc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 0.0, False, False, {'prob': 0.3333333333333333})\n",
            "State: 4, Value: 0.02\n",
            "State: 0, Value: 0.01\n",
            "State: 1, Value: 0.01\n",
            "State: 10, Value: 0.14\n",
            "State: 6, Value: 0.04\n",
            "State: 3, Value: 0.01\n",
            "State: 2, Value: 0.02\n",
            "State: 9, Value: 0.08\n",
            "State: 8, Value: 0.04\n",
            "State: 13, Value: 0.15\n",
            "State: 14, Value: 0.42\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True)\n",
        "\n",
        "\n",
        "env.reset()\n",
        "print(env.step(0))\n",
        "\n",
        "# Define a random policy\n",
        "def policy(state):\n",
        "    return env.action_space.sample()\n",
        "\n",
        "# Run the first-visit Monte Carlo prediction\n",
        "value_function = first_visit_mc_prediction(env, policy, num_episodes=10000)\n",
        "\n",
        "# Print the estimated value function\n",
        "for state, value in value_function.items():\n",
        "    print(f\"State: {state}, Value: {value:.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}